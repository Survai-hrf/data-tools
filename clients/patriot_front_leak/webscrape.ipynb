{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web scrape unicornriot pages for all videos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import html5lib\n",
    "import os, os.path\n",
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "OUT_ROOT_PATH = '/media/beanbagthomas/AED47533D474FEC1/Large_Storage/data/patriotfrontleak/'\n",
    "original_link = 'http://vault.unicornriot.ninja/patriotfrontleaks/'\n",
    "\n",
    "\n",
    "#change to True if you want to run download_visual_media function\n",
    "download_media = False\n",
    "# used if download_media is true - specifies extensions you want to download\n",
    "filetype_to_download = [\n",
    "    '.WEBM', '.MPG', '.MP2', '.MPEG', '.MPE', '.MPV', \n",
    "    '.OGG', '.MP4', '.M4P', '.M4V', '.AVI', '.WMV', \n",
    "    '.MOV', '.QT ', '.FLV', '.SWF' \n",
    "    ]\n",
    "\n",
    "\n",
    "#change to True if you want to run create a csv with all files\n",
    "create_a_csv = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_open_wb(path):\n",
    "    ''' Open \"path\" for writing, creating any parent directories as needed.\n",
    "    '''\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    return open(path, 'wb')\n",
    "\n",
    "\n",
    "def download_visual_media(full_link, file_name, ftd=filetype_to_download, out_path=OUT_ROOT_PATH):\n",
    "    '''download images and video, extensions passed in ftd\n",
    "    '''\n",
    "    r = requests.get(full_link, stream=True)\n",
    "\n",
    "    with safe_open_wb(OUT_ROOT_PATH + file_name) as f:\n",
    "        print(f\"downloading: {file_name}\")\n",
    "        for chunk in r.iter_content(chunk_size=1024*1024):\n",
    "            if chunk:\n",
    "                f.write(chunk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"Returns all scraped files with preserved file hierarchy\n",
    "\n",
    "DESCRIPTION:\n",
    "This program is custom designed for the unicornriot patriotfront leaks to parse\n",
    "and download the vast dataset. Customizable by file extension.\n",
    "\n",
    "TODO\n",
    "Test on nonvideo, images, docs\n",
    "\"\"\"\n",
    "\n",
    "link = original_link\n",
    "traversed = []\n",
    "link_start = \"http://vault.unicornriot.ninja/patriotfrontleaks/\"\n",
    "\n",
    "if create_a_csv:\n",
    "    df = pd.DataFrame(columns=['file name', 'directory path', 'link'])\n",
    "\n",
    "while True:\n",
    "    \n",
    "    req = requests.get(link)\n",
    "    soup = BeautifulSoup(req.text, \"html5lib\")\n",
    "\n",
    "    #print(link)\n",
    "    query = soup.findAll('a')\n",
    "    current_dir_contents = []\n",
    "\n",
    "    # iterate through page and grab first link\n",
    "    for a_tag in query:\n",
    "        a_link = a_tag['href']\n",
    "        \n",
    "        full_link = link + a_link\n",
    "        file_name = full_link.split(link_start)[1]\n",
    "\n",
    "        #download media if in filetype_to_download\n",
    "        if any(ext in a_link.upper() for ext in filetype_to_download):\n",
    "            if download_media == True:\n",
    "                download_visual_media(full_link=full_link, file_name=file_name)\n",
    "\n",
    "        if a_link != '../':\n",
    "            current_dir_contents.append(full_link)\n",
    "\n",
    "            if create_a_csv and '/' not in a_link and '.' in a_link:\n",
    "                data_entry = {'file name': a_link.replace('%', ' '), \n",
    "                    'directory path': file_name.replace('%', ' '), 'link': full_link}\n",
    "\n",
    "                df = df.append(data_entry, ignore_index=True)\n",
    "\n",
    "        # make sure its actual directory and has not been traversed yet, then traverse\n",
    "        if '/' in a_link and full_link not in traversed and a_link != \"../\":  \n",
    "            link += a_link\n",
    "            break\n",
    "\n",
    "    #make list of only links\n",
    "    current_dir_links = [string for string in current_dir_contents if '/' in string[-1]]   \n",
    "\n",
    "    #break loop if at original link and all current_dir_links in traversed\n",
    "    if link == original_link and all(elem in traversed for elem in current_dir_links):\n",
    "        break\n",
    "\n",
    "    # if bottom of a dir is reached or all have already been traversed, add to traversed\n",
    "    if len(current_dir_links) == 0 or all(elem in traversed for elem in current_dir_links):  \n",
    "        traversed.append(link)\n",
    "        link = link.rsplit('/', 2)[0] + '/'\n",
    "        continue\n",
    "\n",
    "df.to_csv(OUT_ROOT_PATH + \"patriot_front_leak.csv\", index_label='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Format discord leaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('SouthernFront.txt') as jsonfile:\n",
    "    file = json.loads(jsonfile.read())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = {}\n",
    "channels = {}\n",
    "\n",
    "file.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "userlist = file['meta']['users']\n",
    "channellist = file['meta']['channels']\n",
    "\n",
    "i = 0\n",
    "for userid in userlist:\n",
    "    users[i] = {'name': userlist[userid]['name'], 'id': userid, 'pointer': i}\n",
    "    i += 1\n",
    "for channelid in channellist:\n",
    "    channels[channellist[channelid]['name']] = channelid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file['data'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate through each channel\n",
    "for channel in file['data'].keys():\n",
    "    full_channel = file['data'][channel]\n",
    "\n",
    "    #iterate through each message\n",
    "    for messageid in full_channel.keys():\n",
    "        print(full_channel[messageid])\n",
    "\n",
    "        #generate time\n",
    "        timeepoch = full_channel[messageid]['t'] / 1000\n",
    "        utc_time = datetime.datetime.utcfromtimestamp(timeepoch).strftime(\"%Y/%d/%m, %H:%M:%S\")\n",
    "        print(utc_time)\n",
    "\n",
    "        #find user\n",
    "        cur_user = users[full_channel[messageid]['u']]['name']\n",
    "        \n",
    "        \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "439b39d12d78a9279461302b80d21b2f249f6fb75eaa40c59538590dc7ee8cb7"
  },
  "kernelspec": {
   "display_name": "Python 3.7.12 ('dsbasic')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
